<!DOCTYPE html>
<html>
	<head>
		<title>Minyi Bond</title>
		<!-- link to main stylesheet -->
		<link rel="stylesheet" type="text/css" href="/css/main.css">
	</head>
  
  
	<body>
		<!--<nav>
    		<ul>
        		<li><a href="/">Home</a></li>
	        	<li><a href="/about">About</a></li>
        		<li><a href="/cv">CV</a></li>
        		<li><a href="/blog">Blog</a></li>
    		</ul>
		</nav>-->
		
		<div class="container">
    		<div class="blurb">
        		<h1> Experience </h1>
        

<h2> Does Tenure Impact Malicious Alerts, June 2019</h2>
<p>  This is a group project from my capstone class. GE Digital – Aviation provides us datasets. 
	GE use Digital Guardian to monitor their employees’ computer activities. Digital Guardian collect 
	the information. Each employee is given a heat score. If the score is above a critical level, then 
	a heat alert will be sent. Analyst will determine whether the alert is malicious. We use the information 
	GE provides to find out the relationship between employee tenure and malicious alerts. My main responsibility 
	in the group project is the model section. 
        (<a href="/GEpaper.pdf" target="_blank">paper</a>; <a href="/GEppt.pdf" target="_blank">slides</a>; 
	<a href="https://github.com/minyibond/minyibond.github.io/blob/main/GEwork_years.ipynb" target="_blank">model</a>)
</p>		
Skills: data cleaning, data exploration, data analysis, Python, group work, experiential capstone course project </p>			
			
			
<h2> Nicotine Consumption Prediction, March 2019</h2>
<p>  I practice predictive analytics on the drug consumption data set provided by UCI machine learning repository. 
	The project is to predict whether a user is an active nicotine user. It is a binary problem. In Jupyter Notebook: 
	1) clean the data; 2) split the data into different data sets for three different uses (training, validation, test);
	3) use different machine learning models to train the data; 4) evaluate the performance of different models; 
	5) improve the model; 6) pick the best model and further evaluate it. During the analysis, I found some patterns 
	of active users and non-active users.
        (<a href="https://github.com/minyibond/minyibond.github.io/blob/main/Nicotine.ipynb" target="_blank">project</a>; 
	<a href="/Nicotine.pdf" target="_blank">slides</a>;
	<a href="https://archive.ics.uci.edu/ml/datasets/Drug+consumption+%28quantified%29" target="_blank">data</a>)
</p>		
Skills: data cleaning, data exploration, data analysis, predictive analytics, Python </p>	
			
			
	
<h2> Guaranteed Social Campaign Based on Targeted Advertising, December 2018   </h2>
<p> Viacom want to offer Guaranteed Social Campaign to their advertisers. For example, Viacom want to guarantee their advertisers 
	that their ads will reach a certain number of views (e.g. 1000 impressions) over a certain period (e.g. a week). Viacom 
	want to charge advertisers $25 per 1000 impressions. This project is to find out whether Guaranteed Social Campaign could be profitable for Viacom.
	(<a href="/Viacom.pdf" target="_blank">slides</a>)	
</p>			
Skills: data cleaning, data exploration, data analysis, R, Tableau, business strategy, experiential learning course project </p>
	
	
<h2> Create Word Cloud from TXT File, June 2018  </h2>
<p> This is a small project that I do when I first learn about Python. Since most of my projects are about structured dataset, 
	I pick the unstructured data to work on in this project. In this project, I generated word cloud graph in cow shape based 
	on the information in the txt file. The word cloud graph displays the more frequent words in bigger font size and the less 
	frequent words in smaller font size.
	(<a href="https://github.com/minyibond/minyibond.github.io/blob/main/word.ipynb" target="_blank">project</a>)
</p>		
Skills: unstructured data, data cleaning, data exploration, data analysis, Python </p>	

			
			
<h2> Default Risk Detection, June 2018 </h2>
<p> This project is based on the ongoing competition “Home Credit Default Risk” on Kaggle.com. 
	Home Credit Group (HCG) focuses on lending money to people with little or no credit history. 
	They try to help those people while they also want to make sure those people can repay their debt on time. 
	In the project, I use supervised learning methods, such as decision trees, random forest, and logistic regression 
	to find out the characters of two groups of people: people who pay their debt on time and people who have default risk.	
        (<a href="/risk.pdf" target="_blank">paper</a>; 
	<a href="/riskppt.pdf" target="_blank">slides</a>; 
	<a href="/riskR.pdf" target="_blank">R codes</a>; 
	<a href="https://www.kaggle.com/c/home-credit-default-risk" target="_blank">data</a>)
</p>		
Skills: data cleaning, data exploration, data analysis, data mining, R </p>	
			
			
			
<h2> An Empirical Analysis of Tariff Binding, May 2013 </h2>
<p> Binding designed by WTO provides some countries trade policy flexibility and restricts some countries to low flexibility. 
	Bindings vary a lot across countries and sectors. The Beshkar, Bond and Rho model, concerning the mechanism of setting optimal binding, 
	yields some strong predictions and evidence within 66 WTO members. My paper is to further check whether their theory still holds using data 
	from subsample and individual country. I expectedly find that bindings increase in political pressure, and decrease in market power and development 
	level within 14 selected countries. The results are not robust to each individual country. Moreover, the results also support the theory that binding 
	is independent of market power when market power is greater than a certain level. An additional finding is that weighted import concentration outside 
	their theory also has a significant and negative effect on binding.
        (<a href="/binding.pdf" target="_blank">paper</a>; <a href="/bindingppt.pdf" target="_blank">slides</a>) 
</p>		
Skills: data cleaning, data exploration, data analysis, Stata, Latex, economics </p>	
			
        
			
			
	</body>
</html>


